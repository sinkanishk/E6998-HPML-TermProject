# E6998-HPML-TermProject

## The Secret Sauce: How Post-Training Optimization Can SuperCharge your AI

The overall objective of this project is to optimize inference of pre-trained large visual-language models without significantly sacrificing their performance

Key Objectives 
- Accelerated Inference
- Model Compressions
- Multi-modal Processing

### Data

The model was evaluated on the benchmark datasets ImageNet and CIFAR100. ImageNet is a large-scale image classification dataset with over 1.4 million images and 1000 object categories. CIFAR100 is a subset of the Tiny Images dataset which contains 60,000 32x32 color images and 100 classes.

### Evaluation

In this project we evaluate the model using both task specific and generic metrics namely

- Accuracy
- Compression Ratio
- Inference Time

### References

[1] Hsu, Y.-C. et al. (2022) “Language Model Compression with Weighted Low-Rank Factorization”, International Conference on Learning Representations. Available at: https://doi.org/10.48550/arXiv.2207.00112. 

[2] Tay, Y. et al. (2022) “Efficient transformers: A survey,” ACM Computing Surveys, 55(6), pp. 1–28. Available at: https://doi.org/10.1145/3530811 

[3] Radford, A. et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. arXiv preprint arXiv:2103.00020. https://doi.org/10.48550/arXiv.2103.00020.

[4] Li, J. et al. (2022) "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation" Available at: https://arxiv.org/pdf/2201.12086.pdf
